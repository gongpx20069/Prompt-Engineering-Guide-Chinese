# 高级 Prompting  
到目前为止，应该很明显，改进 Prompt 可以在不同任务上获得更好的结果。这就是 Prompt 工程背后的整个理念。  
   
虽然这些例子很有趣，但在我们进入更高级的概念之前，让我们更正式地介绍一些概念。  
   
主题：  
   
- [Zero-shot Prompting](#zero-shot-prompting)  
- [Few-shot Prompting](#few-shot-prompting)  
- [Chain-of-Thought Prompting](#chain-of-thought-prompting)  
- [Zero-shot CoT](#zero-shot-cot)  
- [自洽性(Self-Consistency)](#self-consistency)  
- [生成知识 Prompting](#generated-knowledge-prompting)  
- [自动 Prompt 工程师 (APE)](#automatic-prompt-engineer-ape)  
   
---  
## Zero-shot Prompting  
如今的 LLMs 经过大量数据训练并调整以遵循指令，能够零次射击(zero-shot)地执行任务。我们在上一节尝试了一些 zero-shot 示例。这是我们使用的一个示例：  
   
*Prompt:*  
```  
将文本分类为中性、负面或正面。  
   
文本：我认为度假还可以。  
情绪：  
```  
   
*Output:*  
```  
中性  
```  
   
请注意，在上面的 Prompt 中，我们没有向模型提供任何示例——这就是 zero-shot 能力的作用。当 zero-shot 不起作用时，建议在 Prompt 中提供示范或示例。下面我们将讨论称为 few-shot prompting 的方法。  
   
---  
## Few-shot Prompting  
   
尽管大型语言模型已经展示了令人瞩目的 zero-shot 能力，但在使用 zero-shot 设置时，它们在更复杂的任务上仍然表现不足。为了改进这一点，使用 few-shot prompting 技术来实现上下文中的学习，我们在 Prompt 中提供示范来引导模型更好地执行。这些示范作为条件，用于后续示例，我们希望模型生成响应。  
   
让我们通过 [Brown et al. 2020](https://arxiv.org/abs/2005.14165) 提出的一个示例来演示 few-shot prompting。在这个示例中，任务是在句子中正确使用一个新词。  
   
*Prompt:*  
```  
"whatpu" 是坦桑尼亚的一种小型毛茸茸的动物。使用单词 whatpu 的一个句子示例是：  
我们在非洲旅行时看到了这些非常可爱的 whatpus。  
"farduddle" 的意思是快速上下跳动。使用单词 farduddle 的一个句子示例是：  
```  
   
*Output:*  
```  
当我们赢得比赛时，我们都开始 farduddle 以庆祝。  
```  
   
我们可以观察到，模型通过仅提供一个示例（即 1-shot）就学会了如何执行任务。对于更困难的任务，我们可以尝试增加示范（例如 3-shot、5-shot、10-shot 等）。  
   
根据 [Min et al. (2022)](https://arxiv.org/abs/2202.12837) 的发现，以下是一些关于进行 few-shot 时的示范/示例的提示：  
   
- “示范指定的标签空间和输入文本的分布都很重要（无论个别输入的标签是否正确）”  
- 你使用的格式也在性能中起着关键作用，即使你只使用随机标签，这也比根本没有标签要好得多。  
- 进一步的结果显示，从真实标签分布（而不是均匀分布）中选择随机标签也有帮助。  
   
让我们尝试一些示例。让我们首先尝试一个随机标签的示例（意味着标签 Negative 和 Positive 是随机分配给输入的）：  
   
*Prompt:*  
```  
这太棒了！ // 负面  
这很糟糕！ // 正面  
哇，那部电影太酷了！ // 正面  
多么糟糕的节目！ //  
```  
   
*Output:*  
```  
负面  
```  
   
尽管标签被随机化了，我们仍然得到了正确的答案。请注意，我们还保持了格式，这也有帮助。事实上，通过进一步的实验，似乎我们正在实验的新版 GPT 模型甚至对随机格式也变得更加鲁棒。示例：  
   
*Prompt:*  
```  
正面 这太棒了！  
这很糟糕！ 负面  
哇，那部电影太酷了！  
正面  
多么糟糕的节目！ --  
```  
   
*Output:*  
```  
负面  
```  
   
上面的格式没有一致性，但模型仍然预测了正确的标签。我们必须进行更彻底的分析，以确认这是否适用于不同和更复杂的任务，包括不同变化的 Prompts。  
   
### Few-shot Prompting 的局限性  
   
标准的 few-shot prompting 对许多任务都很有效，但仍然不是一种完美的技术，尤其是在处理更复杂的推理任务时。让我们演示为什么会这样。还记得我们之前提供的以下任务吗：  
   
```  
这组中的奇数加起来是一个偶数：15, 32, 5, 13, 82, 7, 1.  
   
A:   
```  
   
如果我们再次尝试，模型会输出以下内容：  
   
```  
是的，这组中的奇数加起来是107，这是一个偶数。  
```  
   
这不是正确的回答，这不仅突出了这些系统的局限性，还表明需要更先进的 Prompt 工程。  
   
让我们尝试添加一些示例，看看 few-shot prompting 是否能改善结果。  
   
*Prompt:*  
```  
这组中的奇数加起来是一个偶数：4, 8, 9, 15, 12, 2, 1.  
A: 答案是假的。  
   
这组中的奇数加起来是一个偶数：17, 10, 19, 4, 8, 12, 24.  
A: 答案是真的。  
   
这组中的奇数加起来是一个偶数：16, 11, 14, 4, 8, 13, 24.  
A: 答案是真的。  
   
这组中的奇数加起来是一个偶数：17, 9, 10, 12, 13, 4, 2.  
A: 答案是假的。  
   
这组中的奇数加起来是一个偶数：15, 32, 5, 13, 82, 7, 1.  
A:   
```  
   
*Output:*  
```  
答案是真的。  
```  
   
那没有用。看来 few-shot prompting 不足以获得这种类型推理问题的可靠响应。上面的示例提供了任务的基本信息。如果你仔细观察，我们介绍的任务类型涉及更多的推理步骤。换句话说，如果我们将问题分解成步骤并向模型展示，可能会有所帮助。最近，[chain-of-thought (CoT) prompting](https://arxiv.org/abs/2201.11903) 已被推广用于解决更复杂的算术、常识和符号推理任务。  
   
总的来说，似乎提供示例对解决某些任务很有用。当 zero-shot prompting 和 few-shot prompting 不够用时，可能意味着模型学到的东西不足以在任务上表现良好。从这里开始，建议开始考虑微调模型或尝试更先进的 prompting 技术。接下来我们将讨论一种叫做 chain-of-thought prompting 的流行技术，它已经获得了很多关注。  
   
---  
   
## Chain-of-Thought Prompting  
   
在 [Wei et al. (2022)](https://arxiv.org/abs/2201.11903) 中介绍的 chain-of-thought (CoT) prompting 通过中间推理步骤实现复杂的推理能力。你可以将它与 few-shot prompting 结合起来，在需要推理之后再回应的更复杂任务上获得更好的结果。  
   
*Prompt:*  
```  
这组中的奇数加起来是一个偶数：4, 8, 9, 15, 12, 2, 1.  
A: 将所有奇数（9, 15, 1）加起来得到25。答案是假的。  
   
这组中的奇数加起来是一个偶数：17, 10, 19, 4, 8, 12, 24.  
A: 将所有奇数（17, 19）加起来得到36。答案是真的。  
   
这组中的奇数加起来是一个偶数：16, 11, 14, 4, 8, 13, 24.  
A: 将所有奇数（11, 13）加起来得到24。答案是真的。  
   
这组中的奇数加起来是一个偶数：17, 9, 10, 12, 13, 4, 2.  
A: 将所有奇数（17, 9, 13）加起来得到39。答案是假的。  
   
这组中的奇数加起来是一个偶数：15, 32, 5, 13, 82, 7, 1.  
A:  
```  
   
*Output:*  
```  
将所有奇数（15, 5, 13, 7, 1）加起来得到41。答案是假的。  
```  
   
哇！当我们提供了推理步骤时，我们可以看到一个完美的结果。我们可以通过提供更少的示例来解决这个任务，即只需要一个示例就足够了：  
   
*Prompt:*  
```  
这组中的奇数加起来是一个偶数：4, 8, 9, 15, 12, 2, 1.  
A: 将所有奇数（9, 15, 1）加起来得到25。答案是假的。  
   
这组中的奇数加起来是一个偶数：15, 32, 5, 13, 82, 7, 1.  
A:  
```  
   
*Output:*  
```  
将所有奇数（15, 5, 13, 7, 1）加起来得到41。答案是假的。  
```  
   
请记住，作者声称这是一个随着足够大的语言模型出现的新能力。  
   
---  
   
## Zero-shot CoT  
   
最近出现的一个想法是 [zero-shot CoT](https://arxiv.org/abs/2205.11916)（Kojima et al. 2022），它本质上涉及在原始 Prompt 中添加“让我们一步一步思考”。让我们尝试一个简单的问题，看看模型的表现如何：  
   
*Prompt:*  
```  
我去市场买了10个苹果。我给邻居2个苹果，给修理工2个苹果。然后我又买了5个苹果，吃了1个。我剩下多少个苹果？  
```  
   
*Output:*  
```  
11个苹果  
```  
   
答案是错误的！现在让我们尝试特殊的 Prompt。  
   
*Prompt:*  
```  
我去市场买了10个苹果。我给邻居2个苹果，给修理工2个苹果。然后我又买了5个苹果，吃了1个。我剩下多少个苹果？  
   
让我们一步一步思考。  
```  
   
*Output:*  
```  
首先，你有10个苹果。  
你给了邻居2个苹果，给修理工2个苹果，所以你剩下了6个苹果。  
然后你又买了5个苹果，所以现在你有11个苹果。  
最后，你吃了1个苹果，所以你会剩下10个苹果。  
```  
   
这个简单的 Prompt 在这个任务上有效是令人印象深刻的。这在你没有太多示例可用于 Prompt 时特别有用。  
   
---  
   
## 自洽性(Self-Consistency)  
   
也许目前为 Prompt 工程最先进的技术之一是自洽性。由 [Wang et al. (2022)](https://arxiv.org/pdf/2203.11171.pdf) 提出的自洽性旨在“替代在 chain-of-thought prompting 中使用的天真贪婪解码”。这个想法是通过 few-shot CoT 采样多个不同的推理路径，并使用生成的答案来选择最一致的答案。这有助于提高 CoT prompting 在涉及算术和常识推理任务的性能。  
   
让我们尝试以下算术推理的示例：  
   
*Prompt:*  
```  
当我6岁时，我的妹妹是我的一半大。现在我70岁，我的妹妹多大了？  
```  
   
*Output:*  
```  
35  
```  
   
输出是错误的！我们如何用自洽性改进这一点？让我们尝试一下。我们将使用 Wang et al. 2022 (表17) 中的 few-shot 示例：  
   
*Prompt:*  
```  
Q: 果园里有15棵树。果园工人今天将在果园里种树。他们完成后，将有21棵树。果园工人今天种了多少棵树？  
A: 我们从15棵树开始。后来我们有21棵树。差异必须是他们种的树的数量。所以，他们必须种了21 - 15 = 6棵树。答案是6。  
   
Q: 如果停车场有3辆车，又来了2辆车，停车场里有多少辆车？  
A: 停车场已经有3辆车了。又来了2辆。现在有3 + 2 = 5辆车。答案是5。  
   
Q: Leah有32块巧克力，她的妹妹有42块。如果他们吃了35块，他们总共还剩多少块？  
A: Leah有32块巧克力，Leah的妹妹有42块。这意味着最初有32 + 42 = 74块巧克力。吃了35块。所以总共还剩74 - 35 = 39块巧克力。答案是39。  
   
Q: Jason有20个棒棒糖。他给了Denny一些棒棒糖。现在Jason有12个棒棒糖。Jason给了Denny多少个棒棒糖？  
A: Jason有20个棒棒糖。既然他现在只有12个，他一定把剩下的都给了Denny。他给Denny的棒棒糖数量一定是20 - 12 = 8个棒棒糖。答案是8。  
   
Q: Shawn有五个玩具。圣诞节时，他从妈妈和爸爸那里各得到了两个玩具。他现在有多少个玩具？
A: 他有5个玩具。他从妈妈那里得到2个，所以之后他有5 + 2 = 7个玩具。然后他又从爸爸那里得到2个，所以总共他有7 + 2 = 9个玩具。答案是9。

Q: 服务器房间里有九台电脑。从星期一到星期四，每天都安装了五台电脑。现在服务器房间里有多少台电脑？
A: 从星期一到星期四有4天。每天增加了5台电脑。这意味着总共增加了4 * 5 = 20台电脑。一开始有9台电脑，所以现在有9 + 20 = 29台电脑。答案是29。

Q: Michael有58个高尔夫球。星期二，他丢了23个高尔夫球。星期三，他又丢了2个。星期三结束时他还剩多少个高尔夫球？
A: Michael最初有58个球。星期二他丢了23个，所以之后他有58 - 23 = 35个球。星期三他又丢了2个，所以现在他有35 - 2 = 33个球。答案是33。

Q: Olivia有23美元。她买了五个3美元一个的贝果。她还剩多少钱？
A: 她买了5个贝果，每个3美元。这意味着她花了5

Q: 当我6岁时，我的妹妹是我的一半大。现在我70岁，我的妹妹多大了？
A:
```

*Output 1:*
```
当我6岁时，我的妹妹是我的一半大，所以她是3岁。现在我70岁，所以她是70 - 3 = 67岁。答案是67岁。
```

*Output 2:*
```
当叙述者6岁时，他的妹妹是他年龄的一半，也就是3岁。既然叙述者现在70岁，他的妹妹将是70 - 3 = 67岁。答案是67岁。
```

*Output 3:*
```
当我6岁时，我的妹妹是我的一半大，所以她是3岁。现在我70岁，所以她是70/2 = 35岁。答案是35岁。
```

计算最终答案涉及几个步骤（详情请查阅论文），但为了简单起见，我们可以看到已经有一个多数答案出现，这本质上将成为最终答案。

---
## 生成知识 Prompting

LLMs 持续得到改进，一种流行的技术包括能够纳入知识或信息，以帮助模型做出更准确的预测。

使用类似的想法，模型也可以用来生成知识，然后再做出预测吗？这就是 [Liu et al. 2022](https://arxiv.org/pdf/2110.08387.pdf) 论文尝试的——生成知识，作为 Prompt 的一部分。特别是，这对于常识推理等任务有多有帮助？

让我们尝试一个简单的 Prompt：

*Prompt:*
```
高尔夫的一部分是试图获得比其他人更高的总分。是或否？
```

*Output:*
```
是。
```

这种错误揭示了 LLMs 执行需要更多关于世界知识的任务的局限性。我们如何通过生成知识来改进这一点？

首先，我们生成一些“知识”：

*Prompt:*
```
输入：希腊比墨西哥大。
知识：希腊的面积约为131,957平方公里，而墨西哥的面积约为1,964,375平方公里，使墨西哥比希腊大1,389%。

输入：眼镜总是起雾。
知识：当你的汗水、呼吸和周围湿度中的水蒸气落在一个冷表面上，冷却后，然后变成微小的液滴，形成一层你看到的雾。你的镜片相对于你的呼吸会比较凉，特别是当外面的空气很冷时。

输入：鱼能思考。
知识：鱼比它们看起来更聪明。在许多领域，如记忆，它们的认知能力与包括非人类灵长类动物在内的“高级”脊椎动物相当或超过。鱼的长期记忆帮助它们跟踪复杂的社会关系。

输入：一生中吸很多烟的常见后果是患肺癌的几率高于正常水平。
知识：那些一生中平均每天少于一支烟的人，死于肺癌的风险是从不吸烟者的九倍。对于那些每天吸1到10支烟的人来说，死于肺癌的风险几乎是从不吸烟者的12倍。

输入：石头和卵石一样大。
知识：根据沉积学的Udden-Wentworth量表，卵石是一种岩石碎片，粒径为4到64毫米。卵石通常被认为比粒径为2到4毫米的颗粒要大，比粒径为64到256毫米的卵石要小。

输入：高尔夫的一部分是试图获得比其他人更高的总分。
知识：
```

*Knowledge 1:*
```
高尔夫的目标是以最少的击球数打完一组洞。一轮高尔夫通常包括18个洞。每个洞在标准高尔夫球场的一轮比赛中只打一次。每次击球记为一分，总击球数用来确定比赛的胜者。
```

*Knowledge 2:*
```
高尔夫是一项精密的球杆与球运动，参赛者（或高尔夫球手）使用多种类型的球杆将球击入球场上的一系列洞中，尽可能少的击球次数。目标是以最低分数完成球场，这是通过将每个洞的总击球数相加计算出 来的。得分最低的球员赢得比赛。
```

我们使用了 [Liu et al. 2022](https://arxiv.org/pdf/2110.08387.pdf) 论文中提供的 Prompt。

下一步是整合知识并得出预测。我将问题重新格式化为 QA 格式，以指导答案格式。

*Prompt:*
```
问题：高尔夫的一部分是试图获得比其他人更高的总分。是或否？

知识：高尔夫的目标是以最少的击球数打完一组洞。一轮高尔夫通常包括18个洞。每个洞在标准高尔夫球场的一轮比赛中只打一次。每次击球记为一分，总击球数用来确定比赛的胜者。

解释并回答：
```

*Answer 1 (非常有信心):*
```
不，高尔夫的目标不是获得比其他人更高的总分。相反，目标是以最少的击球数打完一组洞。用总击球数来确定比赛的胜者，而不是总分数。
```


```
问题：高尔夫的一部分是试图获得比其他人更高的总分。是或否？

知识：高尔夫是一项精密的球杆与球运动，参赛者（或高尔夫球手）使用多种类型的球杆将球击入球场上的一系列洞中，尽可能少的击球次数。目标是以最低分数完成球场，这是通过将每个洞的总击球数相加 计算出来的。得分最低的球员赢得比赛。

解释并回答：
```

*Answer 2 (信心较低):*
```
是的，高尔夫的一部分是试图获得比其他人更高的总分。每个球员都试图以最低分数完成球场，这是通过将每个洞的总击球数相加计算出来的。得分最低的球员赢得比赛。
```

这个示例发生了一些非常有趣的事情。在第一个答案中，模型非常有信心，但在第二个答案中并不是那么有信心。我简化了演示过程，但在得出最终答案时还有一些更多的细节需要考虑。更多详情请查看论文 。

---

## 自动 Prompt 工程师 (APE)

![](../img/APE.png)

[Zhou et al., (2022)](https://arxiv.org/abs/2211.01910) 提出了自动 Prompt 工程师 (APE)，一个用于自动生成和选择指令的框架。指令生成问题被构架为自然语言合成问题，使用 LLMs 生成和搜索候 选解决方案，将其视为黑盒优化问题。

第一步涉及一个大型语言模型（作为推理模型），它被给予输出示范来为任务生成指令候选。这些候选解决方案将指导搜索程序。指令使用目标模型执行，然后根据计算出的评估分数选择最合适的指令。     

APE 发现了比人类工程师设计的 "Let's think step by step" 提示（Kojima et al., 2022）更好的 zero-shot CoT 提示。

提示 "Let's work this out in a step by step way to be sure we have the right answer." 引发了推理链，提高了在 MultiArith 和 GSM8K 基准测试上的性能：

![](../img/ape-zero-shot-cot.png)

这篇论文触及了与 Prompt 工程相关的一个重要话题，即自动优化 Prompts 的想法。虽然我们在这个指南中没有深入讨论这个话题，但如果你对这个话题感兴趣，这里有一些关键论文：

- [AutoPrompt](https://arxiv.org/abs/2010.15980) - 提出了一种基于梯度引导搜索自动为多种任务创建 Prompts 的方法。
- [Prefix Tuning](https://arxiv.org/abs/2101.00190) - 提出了一种轻量级的微调替代方案，它为自然语言生成任务前置了一个可训练的连续前缀。
- [Prompt Tuning](https://arxiv.org/abs/2104.08691) - 提出了一种通过反向传播学习软提示的机制。

---
[上一节 (基础 Prompting)](./prompts-basic-usage.md)

[下一节 (应用)](./prompts-applications.md)