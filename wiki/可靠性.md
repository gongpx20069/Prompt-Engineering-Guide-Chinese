## 可靠性  
   
我们已经看到，使用像 few-shot 学习这样的技术，精心制作的 Prompts 对于各种任务是多么有效。当我们考虑在 LLMs 之上构建实际应用程序时，思考这些语言模型的可靠性变得至关重要。本指南旨在展示有效的 Prompt 技术，以提高像 GPT-3 这样的 LLMs 的可靠性。感兴趣的话题包括泛化性、校准、偏见、社会偏见和事实性等。  
   
**请注意，本节正在积极开发中。**  
   
主题：  
- [事实性](#事实性)  
- [偏见](#偏见)  
- ...  
   
---  
## 事实性  
LLMs 有时会生成听起来连贯且令人信服的回应，但有时可能是编造的。改进 Prompts 可以帮助模型生成更准确/事实性的回应，并减少生成不一致和虚构回应的可能性。  
   
一些解决方案可能包括：  
- 提供真实情况（例如，作为上下文的相关文章段落或维基百科条目），以减少模型产生虚构文本的可能性。  
- 配置模型以通过降低概率参数产生较少多样性的回应，并指导它承认（例如，“我不知道”）当它不知道答案时。  
- 在 Prompt 中提供它可能知道和不知道的问题和回应的示例组合  
   
让我们看一个简单的例子：  
   
*Prompt:*  
```  
Q: 什么是原子？  
A: 原子是构成一切的微小粒子。  
   
Q: Alvan Muntz 是谁？  
A: ？  
   
Q: Kozar-09 是什么？  
A: ？Q:  
   
火星有多少个卫星？  
A: 两个，Phobos 和 Deimos。  
   
Q: Neto Beto Roberto 是谁？  
```  
   
*Output:*  
```  
A: ？  
```  
   
我编造了“Neto Beto Roberto”这个名字，所以在这个例子中模型是正确的。尝试改变一下问题，看看你是否能让它工作。根据你到目前为止所学的所有内容，有不同的方法可以进一步改进这一点。  
   
---  
## 偏见  
LLMs 可能会产生潜在有害并显示可能会恶化模型在下游任务上性能的偏见。其中一些可以通过有效的 Prompt 策略来缓解，但可能需要更高级的解决方案，如审查和过滤。  
   
### 示例的分布  
在执行 few-shot 学习时，示例的分布是否会影响模型的性能或以某种方式使模型偏见？我们可以在这里进行一个简单的测试。  
   
*Prompt:*  
```  
Q: 我刚刚得到了有史以来最好的消息！  
A: 正面  
   
Q: 我们在工作中得到了加薪！  
A: 正面  
   
Q: 我为今天所取得的成就感到骄傲。  
A: 正面  
   
Q: 我今天过得非常愉快！  
A: 正面  
   
Q: 我真的很期待周末。  
A: 正面  
   
Q: 我刚刚得到了有史以来最好的礼物！  
A: 正面  
   
Q: 我现在很高兴。  
A: 正面  
   
Q: 我很幸运能有这么棒的家庭。  
A: 正面  
   
Q: 外面的天气很阴沉。  
A: 负面  
   
Q: 我刚刚得到了一些糟糕的消息。  
A: 负面  
   
Q: 那留下了一种苦涩的味道。  
A:  
```  
   
*Output:*  
```  
负面  
```  
   
在上面的例子中，似乎示例的分布并没有使模型产生偏见。这是好的。让我们再试一个难以分类的文本，看看模型如何做：  
   
*Prompt:*  
```  
Q: 这里的食物很好吃！  
A: 正面  
   
Q: 我真的厌倦了这门课程。  
A: 负面  
   
Q: 我不敢相信我考试没通过。  
A: 负面  
   
Q: 我今天过得很开心！  
A: 正面  
   
Q: 我讨厌这份工作。  
A: 负面  
   
Q: 这里的服务太糟糕了。  
A: 负面  
   
Q: 我对我的生活感到非常沮丧。  
A: 负面  
   
Q: 我从来没有休息的机会。  
A: 负面  
   
Q: 这顿饭味道糟糕。  
A: 负面  
   
Q: 我受不了我的老板。  
A: 负面  
   
Q: 我感觉到了什么。  
A:  
```  
   
*Output:*  
```  
负面  
```  
   
虽然最后一句话有些主观，但我改变了分布，使用了 8 个正面示例和 2 个负面示例，然后再次尝试了同样的句子。猜猜模型怎么回应的？它回答了“正面”。模型可能对情感分类有很多了解，因此很难让它显示出这个问题的偏见。这里的建议是避免偏向分布，而是为每个标签提供更平衡的示例数量。对于模型没有太多了解的更难任务，它可能会更加挣扎。  
   
### 示例的顺序  
在执行 few-shot 学习时，顺序是否会影响模型的性能或以某种方式使模型偏见？  
   
你可以尝试上面的示例，看看是否可以通过改变顺序使模型偏向某个标签。建议是随机排列示例。例如，避免先有所有正面的示例，然后是最后的负面示例。如果标签分布不均，则这个问题会进一步放大。始终确保进行大量实验以减少这种类型的偏见性。  
   
---  
   
其他即将到来的主题：  
- 干扰  
- 荒谬的相关性  
- 领域转移  
- 毒性  
- 仇恨言论/攻击性内容  
- 刻板印象偏见  
- 性别偏见  
- 即将推出！  
- 红队攻击  
   
---  
## 参考文献  
- [宪法 AI：来自 AI 反馈的无害性](https://arxiv.org/abs/2212.08073)（2022年12月）  
- [重新思考示例的作用：是什么让语境中的学习起作用？](https://arxiv.org/abs/2202.12837)（2022年10月）  
- [引导 GPT-3 变得可靠](https://arxiv.org/abs/2210.09150)（2022年10月）  
- [让语言模型成为更好的推理者的进展](https://arxiv.org/abs/2206.02336)（2022年6月）  
- [ML 安全中未解决的问题](https://arxiv.org/abs/2109.13916)（2021年9月）  
- [红队攻击语言模型以减少伤害：方法、扩展行为和经验教训](https://arxiv.org/abs/2209.07858)（2022年8月）  
- [StereoSet：测量预训练语言模型中的刻板印象偏见](https://aclanthology.org/2021.acl-long.416/)（2021年8月）  
- [使用前校准：提高语言模型的少数样本性能](https://arxiv.org/abs/2102.09690v2)（2021年2月）  
- [提高可靠性的技术 - OpenAI Cookbook](https://github.com/openai/openai-cookbook/blob/main/techniques_to_improve_reliability.md)  
   
---  
[上一节 (对抗性 Prompting)](./prompts-adversarial.md)  
   
[下一节 (杂项)](./prompts-miscellaneous.md)